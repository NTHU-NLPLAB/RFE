{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './az_papers/paper-structure.dtd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y8/5bg49dt971n_m7qqgdj7rqyr0000gn/T/ipykernel_50272/1561126153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./az_papers/paper-structure.dtd\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdtd_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './az_papers/paper-structure.dtd'"
     ]
    }
   ],
   "source": [
    "with open(\"./az_papers/paper-structure.dtd\") as f:\n",
    "    dtd_soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:00<00:00, 72748.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# parse data structure\n",
    "structure = {}\n",
    "\n",
    "elem_name = \"\"\n",
    "for line in tqdm(dtd_soup):\n",
    "    line = line.string\n",
    "    split_by_space = line.split()\n",
    "    if len(split_by_space):\n",
    "        if split_by_space[0] == \"ATTLIST\":\n",
    "            elem_name = split_by_space[1]\n",
    "            split_by_line = line.split(\"\\n\")\n",
    "            if len(split_by_line) > 1:\n",
    "                structure[elem_name] = [attr_line.split()[0] for attr_line in split_by_line]\n",
    "            else:\n",
    "                structure[elem_name] = [split_by_line[0].split()[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bs_text(line):\n",
    "    \"\"\"\n",
    "    Returns bs datatype, not string!\n",
    "    \"\"\"\n",
    "    if line.eqn:\n",
    "        for eq in line.find_all(\"eqn\"):\n",
    "            eq.replace_with(\"EQN\")\n",
    "    if line.cref:\n",
    "        for ref in line.find_all(\"cref\"):\n",
    "            ref.replace_with(\"CREF\")\n",
    "\n",
    "    replaced_str = line.get_text()\n",
    "    replaced_str = re.sub(\"``\", '\"', replaced_str)\n",
    "    replaced_str = re.sub(\"''\", '\"', replaced_str)\n",
    "    line.string = replaced_str\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1477c0dc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_sents_get_labels(combined_sent):\n",
    "    doc = nlp(combined_sent[\"text\"])\n",
    "    out_sents = []\n",
    "    for sent in doc.sents:\n",
    "        out_sents.append(\n",
    "            {\n",
    "            \"text\": sent.text,\n",
    "            \"label\": combined_sent[\"label\"],\n",
    "            \"s_id\": combined_sent['s_id'],\n",
    "            \"header\": combined_sent['header'],\n",
    "            \"fileno\": combined_sent['fileno'],\n",
    "            }\n",
    "            )\n",
    "    return out_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_DIV = \"=\"*100+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(LINE_DIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all az-scixml files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/82 [00:00<00:03, 23.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/82 [00:00<00:04, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 19/82 [00:01<00:03, 18.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 31/82 [00:01<00:01, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 38/82 [00:01<00:01, 22.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 45/82 [00:02<00:01, 23.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 59/82 [00:02<00:00, 29.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 71/82 [00:03<00:00, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:03<00:00, 24.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "# of no label sentences:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parse all data\n",
    "\n",
    "docs = []\n",
    "\n",
    "print(\"Processing all az-scixml files...\")\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "# of no label sentences:  0\n"
     ]
    }
   ],
   "source": [
    "# test one paper\n",
    "no_label = 0\n",
    "\n",
    "doc = {}\n",
    "with open(os.path.join(\".\", \"az_papers\", \"raw\", \"9405010.az-scixml\")) as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser', from_encoding='iso-8859-1')\n",
    "    #soup = BeautifulSoup(f, features=\"xml\", from_encoding='iso-8859-1')\n",
    "    soup = soup.paper\n",
    "    doc['title'] = soup.title.text\n",
    "    doc['year'] = soup.year.text\n",
    "    doc['fileno'] = soup.fileno.text\n",
    "    doc['classification'] = soup.classification.text\n",
    "    doc['abstract'] = []\n",
    "\n",
    "    abstr = soup.abstract\n",
    "    for line in abstr.find_all('a-s'):\n",
    "        line = clean_bs_text(line)\n",
    "        sent_dict = {'text': line.get_text(), 'label': line.get('az')}\n",
    "        doc['abstract'].append(sent_dict)\n",
    "    \n",
    "    doc['body'] = []\n",
    "    # doc['body'] = [\n",
    "    # {'header':..., 'sentences': [{'text': ..., \"label\": ...}, {...}]}, {...}\n",
    "    # ]\n",
    "    body = soup.body\n",
    "    for div in body.find_all(\"div\"):\n",
    "        header = \"\"\n",
    "        if int(div.get(\"depth\")) > 1: # update header\n",
    "            header = div.header.get_text()\n",
    "            continue\n",
    "        header = header if header else div.header.get_text()\n",
    "        for parag in div.find_all('p'):\n",
    "            body_dict = {}\n",
    "            body_dict['header'] = header\n",
    "            body_dict['sentences'] = []\n",
    "\n",
    "            combined_sent = {\"text\": \"\", \"label\": \"\", \"header\": header, \"fileno\": doc[\"fileno\"],}\n",
    "            for i, line in enumerate(parag.find_all('s')):\n",
    "                s_id = line.get(\"id\")\n",
    "                combined_sent['s_id'] = s_id\n",
    "                combined_sent['header'] = header\n",
    "                line = clean_bs_text(line)\n",
    "                if line.get(\"type\")==\"ITEM\":\n",
    "                    label = line.get('az')\n",
    "                    if not label:\n",
    "                        no_label += 1\n",
    "                        continue\n",
    "                    if combined_sent[\"label\"]:\n",
    "                        if combined_sent[\"label\"] != label:\n",
    "                            combined_sent = split_sent_get_label(combined_sent)\n",
    "                            body_dict['sentences'].extend(combined_sent) # save previous aggregated sents\n",
    "                            combined_sent = {\"text\": line.get_text(), \"label\": label, \"s_id\": s_id, \"header\": header, \"fileno\": doc[\"fileno\"], }\n",
    "                            continue\n",
    "                        else:\n",
    "                            pass # sents with the same label are lumped as one big sentence. \n",
    "                            # this is dealt with in the next elif.\n",
    "                    else:\n",
    "                        combined_sent[\"label\"] = label # first sent in the ITEM list\n",
    "                    combined_sent[\"text\"] = \"\".join([combined_sent[\"text\"], line.get_text()])\n",
    "                    combined_sent[\"s_id\"] = s_id\n",
    "\n",
    "                    if (i==(len(parag.find_all('s'))-1)) and combined_sent[\"text\"]: # the final item is the final line in the parag\n",
    "                        print(\"hello\")\n",
    "                        combined_sent = split_sents_get_labels(combined_sent)\n",
    "                        body_dict['sentences'].extend(combined_sent)\n",
    "                        combined_sent = {\"text\": \"\", \"label\": \"\", \"header\": header, \"fileno\": doc[\"fileno\"]} # reset\n",
    "                    continue\n",
    "                    \n",
    "\n",
    "                elif (line.get(\"type\")!=\"ITEM\") and combined_sent[\"text\"]: # we're out of an ITEM list but still in a paragraph\n",
    "                    print(\"hi\")\n",
    "                    combined_sent = split_sents_get_labels(combined_sent)\n",
    "                    body_dict['sentences'].extend(combined_sent)\n",
    "                    combined_sent = {\"text\": \"\", \"label\": \"\", \"header\": header, \"fileno\": doc[\"fileno\"],} # reset\n",
    "\n",
    "                label = line.get('az') # deal with current line\n",
    "                if not label: \n",
    "                    no_label += 1\n",
    "                    continue\n",
    "                sent_dict = {'text': line.get_text(), 'label': label, \"header\": header, \"s_id\": s_id, \"fileno\": doc[\"fileno\"],}\n",
    "                body_dict['sentences'].append(sent_dict)\n",
    "            if body_dict['sentences']:\n",
    "                if parag.find_all('image'):\n",
    "                    body_dict['sentences'][-1]['text'] = body_dict['sentences'][-1]['text'] + \"[IMAGE]\"\n",
    "                doc['body'].append(body_dict)\n",
    "print(\"# of no label sentences: \", no_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question : ',\n",
       " 'label': 'AIM'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open(\"az_papers_06062023.jsonl\", \"w\") as f:\n",
    "    f.write_all(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'header': ' Introduction ',\n",
       " 'sentences': [{'text': ' In the approach to discourse structure developed in Sidner 1983 and Grosz et al. 1986 , a discourse exhibits both global and local coherence . ',\n",
       "   'label': 'OTH',\n",
       "   'header': ' Introduction ',\n",
       "   's_id': 'S-0',\n",
       "   'fileno': '9410005'},\n",
       "  {'text': ' On this view , a key element of local coherence is centering , a system of rules and constraints that govern the relationship between what the discourse is about and some of the linguistic choices made by the discourse participants , e.g. choice of grammatical function , syntactic structure , and type of referring expression ( proper noun , definite or indefinite description , reflexive or personal pronoun , etc . ) ',\n",
       "   'label': 'OTH',\n",
       "   'header': ' Introduction ',\n",
       "   's_id': 'S-1',\n",
       "   'fileno': '9410005'},\n",
       "  {'text': ' Pronominalization in particular serves to focus attention on what is being talked about ; inappropriate use or failure to use pronouns causes communication to be less fluent . ',\n",
       "   'label': 'OTH',\n",
       "   'header': ' Introduction ',\n",
       "   's_id': 'S-2',\n",
       "   'fileno': '9410005'},\n",
       "  {'text': ' For instance , it takes longer for hearers to process a pronominalized noun phrase that is not in focus than one that is , while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not Guindon 1985 . [IMAGE]',\n",
       "   'label': 'OTH',\n",
       "   'header': ' Introduction ',\n",
       "   's_id': 'S-3',\n",
       "   'fileno': '9410005'}]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az_papers = list(jsonlines.open(\"az_papers_06062023.jsonl\"))\n",
    "az_papers[0]['body'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(jsonlines.open(\"az_papers_06062023.jsonl\"))\n",
    "\n",
    "az_abs = [it[\"abstract\"] for it in docs]\n",
    "\n",
    "with jsonlines.open(\"az_papers_abstract_06062023.jsonl\", \"w\") as f:\n",
    "    f.write_all(az_abs)\n",
    "\n",
    "# doc['body'] = [\n",
    "# {'header':..., 'sentences': [{'text': ..., \"label\": ...}, {...}]}, {...}\n",
    "# ]\n",
    "az_body = [parag['sentences'] for doc in docs for parag in doc[\"body\"]]\n",
    "with jsonlines.open(\"az_papers_body_06062023.jsonl\", \"w\") as f:\n",
    "    f.write_all(az_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_all = az_abs + az_body\n",
    "with jsonlines.open(\"az_papers_all_06062023.jsonl\", \"w\") as f:\n",
    "    f.write_all(az_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst = list(jsonlines.open(\"az_papers_abstract.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_body = [parag['sentences'] for doc in az_papers for parag in doc[\"body\"]]\n",
    "with jsonlines.open(\"az_papers_body.jsonl\", \"w\") as f:\n",
    "    f.write_all(az_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"az_papers/9504033.az-scixml\") as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s az=\"BKG\" id=\"S-1\"> For instance , parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain <ref type=\"P\">Sparck Jones 1983</ref> . </s>\n"
     ]
    }
   ],
   "source": [
    "for line in soup.find_all('s'):\n",
    "    if line.ref:\n",
    "        for r in line.find_all('ref'):\n",
    "            print(line)\n",
    "            r.replace_with(\"CREF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c9922e9dad2b1e0442a0ebcb9382d1d4d0de9ad108fc9ce16ef711bb24c8435"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
